{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf1acae",
   "metadata": {},
   "source": [
    "# <center>Home Assignment 1</center>\n",
    "### <center>Youssef Benhachem</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eaf2c7",
   "metadata": {},
   "source": [
    "### **(5) -** Logitic regression implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62ba5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7420fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"Logistic regression with SGD as the solver.\n",
    "\n",
    "    Example usage:\n",
    "        > clf = LogisticRegression()\n",
    "        > clf.fit(x_train, y_train)\n",
    "        > clf.predict(x_eval)\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.001, max_iter=1000, theta_0=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            learning_rate: Learning rate for sgd\n",
    "            max_iter: Maximum number of iterations for the solver.\n",
    "            theta_0: Initial guess for theta. If None, use the zero vector.\n",
    "        \"\"\"\n",
    "        self.theta = theta_0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        \"\"\"Run SGD to minimize the logistic loss for logistic regression.\n",
    "\n",
    "        Args:\n",
    "            x: Training example inputs. Shape (n_examples, dim).\n",
    "            y: Training example labels. Shape (n_examples,).\n",
    "        \"\"\"\n",
    "        for _ in tqdm(range(self.max_iter)):\n",
    "            for obs, label in zip(x, y):\n",
    "                self.theta += self.learning_rate * label * (1/(1+np.exp(label*np.dot(self.theta,obs)))) * obs\n",
    "    \n",
    "    def fit_reg_l2(self, x, y, lam):\n",
    "        \"\"\"Run SGD with l2-regularization to minimize the logistic loss for logistic regression.\n",
    "\n",
    "        Args:\n",
    "            x: Training example inputs. Shape (n_examples, dim).\n",
    "            y: Training example labels. Shape (n_examples,).\n",
    "        \"\"\"\n",
    "        for _ in tqdm(range(self.max_iter)):\n",
    "            for obs, label in zip(x, y):\n",
    "                self.theta = (1 - 2* lam* self.learning_rate)* self.theta + self.learning_rate * label * (1/(1+np.exp(label*np.dot(self.theta,obs)))) * obs\n",
    "    \n",
    "    def fit_reg_l1(self, x, y, lam):\n",
    "        \"\"\"Run SGD with l1-regularization to minimize the logistic loss for logistic regression.\n",
    "\n",
    "        Args:\n",
    "            x: Training example inputs. Shape (n_examples, dim).\n",
    "            y: Training example labels. Shape (n_examples,).\n",
    "        \"\"\"\n",
    "        for _ in tqdm(range(self.max_iter)):\n",
    "            for obs, label in zip(x, y):\n",
    "                self.theta = self.theta -self.learning_rate*lam*np.sign(self.theta) + self.learning_rate * label * (1/(1+np.exp(label*np.dot(self.theta,obs)))) * obs\n",
    "                \n",
    "                \n",
    "    def predict(self, x):\n",
    "        \"\"\"Return predicted probabilities given new inputs x.\n",
    "\n",
    "        Args:\n",
    "            x: Inputs of shape (n_examples, dim).\n",
    "\n",
    "        Returns:\n",
    "            Outputs of shape (n_examples,).\n",
    "        \"\"\"\n",
    "        return 1/(1+np.exp(-np.dot(x, self.theta)))\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        y_pred = self.predict(x_test)\n",
    "        loss = np.sum(np.log(1+np.exp(-y_test*np.dot(x_test, self.theta))))\n",
    "        return loss/len(y_pred)\n",
    "    \n",
    "    def zero_one_loss(self, x_test, y_test):\n",
    "        y_pred = self.predict(x_test)\n",
    "        loss = np.sum(abs(y_pred - y_test))\n",
    "        return loss/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdffd9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train():\n",
    "    train_data = []\n",
    "    letter2output = {'A':1,'B':-1, 'C':-1}\n",
    "    \n",
    "    for letter in tqdm(['A','B','C']):\n",
    "        train_path = f'data/train/{letter}'\n",
    "        images_names = (os.listdir(train_path))\n",
    "        for img_path in (f'{train_path}/{img_name}' for img_name in images_names):\n",
    "            img = Image.open(img_path)\n",
    "            flat = np.array(img).flatten()\n",
    "            img_vec = np.append(flat/np.linalg.norm(flat), letter2output[letter])\n",
    "            train_data.append(img_vec)\n",
    "    return np.array(train_data)\n",
    "\n",
    "def load_test():\n",
    "    test_data = []\n",
    "    letter2output = {'A':1,'B':-1, 'C':-1}\n",
    "    \n",
    "    for letter in tqdm(['A','B','C']):\n",
    "        test_path = f'data/test/{letter}'\n",
    "        images_names = (os.listdir(test_path))\n",
    "        for img_path in (f'{test_path}/{img_name}' for img_name in images_names):\n",
    "            img = Image.open(img_path)\n",
    "            flat = np.array(img).flatten()\n",
    "            img_vec = np.append(flat/np.linalg.norm(flat), letter2output[letter])\n",
    "            test_data.append(img_vec)\n",
    "    return np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ad07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "train_data =load_train()\n",
    "test_data = load_test()\n",
    "\n",
    "# Randomizing the data\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a31827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the intercept for Linear Models\n",
    "train_data = np.c_[np.ones(6000), train_data]\n",
    "test_data = np.c_[np.ones(750), test_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f476b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing train and test data\n",
    "x_train = train_data[:, :-1]\n",
    "y_train = train_data[:, -1]\n",
    "x_test = test_data[:, :-1]\n",
    "y_test = test_data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8f996e",
   "metadata": {},
   "source": [
    "**(5)-(a)-**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37007b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c12120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_loss(learning_rate, iterations, x_train, y_train, x_test, y_test, classifier):\n",
    "    \"\"\"\n",
    "    \n",
    "        Args:\n",
    "            learning_rate: Learning rate for sgd\n",
    "            iterations : List representing the maximum iterations to fit our model\n",
    "    \"\"\"\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    for it in iterations: \n",
    "        if classifier == \"LogisticRegression\":\n",
    "            clf = LogisticRegression(theta_0 = np.zeros(785), max_iter = it, learning_rate=learning_rate)\n",
    "        elif classifier == \"LinearRegression\":\n",
    "            clf = LinearRegression(theta_0 = np.zeros(785), max_iter = it, learning_rate=learning_rate)\n",
    "        elif classifier == \"Perceptron\":\n",
    "            clf = Perceptron(theta_0 = np.zeros(785), max_iter = it, learning_rate=learning_rate)\n",
    "        clf.fit(x_train, y_train)\n",
    "        train_loss.append(clf.evaluate(x_train, y_train))\n",
    "        test_loss.append(clf.evaluate(x_test, y_test))\n",
    "\n",
    "    return train_loss, test_loss\n",
    "\n",
    "def plot_loss(learning_rates, iterations, x_train, y_train, x_test, y_test, classifier):\n",
    "    train_loss_curves = []\n",
    "    test_loss_curves = []\n",
    "    \n",
    "    for learning_rate in learning_rates:\n",
    "        result = train_test_loss(learning_rate, iterations, x_train, y_train, x_test, y_test, classifier)\n",
    "        \n",
    "        train_loss_curves.append(result[0])\n",
    "        test_loss_curves.append(result[1])\n",
    "        \n",
    "        train_curve, test_curve = result[0], result[1]\n",
    "    \n",
    "    for train_curve, learning_rate in zip(train_loss_curves, learning_rates):\n",
    "        #col = (np.random.random(), np.random.random(), np.random.random())\n",
    "        plt.plot(iterations, train_curve, label=f\"{learning_rate}\")\n",
    "        \n",
    "    plt.title(f\"{classifier}: Training Error\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Training Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "    for test_curve, learning_rate in zip(test_loss_curves, learning_rates):\n",
    "        #cola = (rd.uniform(0,1), rd.uniform(0,1), rd.uniform(0,1))\n",
    "        plt.plot(iterations, test_curve, label=f\"{learning_rate}\")\n",
    "        \n",
    "    plt.title(f\"{classifier} : Testing Error\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Testing Loss\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "        \n",
    "    return train_loss_curves, test_loss_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df2848a",
   "metadata": {},
   "source": [
    "=> The best learning rate is then for Logistic Regression is then $0.001$ followed by $0.01$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94d8075",
   "metadata": {},
   "source": [
    "**(5)-(b)-**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cde340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_theta(iterations, x_train, y_train, classifier):\n",
    "    thetas = []\n",
    "    for it in iterations:\n",
    "        if classifier == \"LogisticRegression\":\n",
    "            clf = LogisticRegression(theta_0 = np.zeros(785), max_iter = it, learning_rate=0.001)\n",
    "        elif classifier == \"LinearRegression\":\n",
    "            clf = LinearRegression(theta_0 = np.zeros(785), max_iter = it, learning_rate=0.001)\n",
    "        elif classifier == \"Perceptron\":\n",
    "            clf = Perceptron(theta_0 = np.zeros(785), max_iter = it, learning_rate=0.001)\n",
    "        elif classifier == \"KNN\":\n",
    "            clf = KNN(5, x_train,y_train)\n",
    "            \n",
    "        clf.fit(x_train, y_train)\n",
    "        thetas.append(clf.theta)\n",
    "    return thetas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0595fa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = save_theta([10,100,1000,10000], x_train, y_train, classifier=\"LogisticRegression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defb18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "def show_thetas(thetas):\n",
    "    for theta in thetas:\n",
    "        theta = np.abs(theta/np.linalg.norm(theta))\n",
    "        theta = theta[1:].reshape(28,28)\n",
    "        im = Image.fromarray(np.uint8(cm.gist_earth(theta)*255))\n",
    "        scaled_img = im.resize((250, 250))\n",
    "        display(scaled_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a3ca98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_thetas(thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe12384",
   "metadata": {},
   "source": [
    "### Linear Least Squares Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cfe9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc736bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"Linear regression with SGD as the solver.\n",
    "\n",
    "    Example usage:\n",
    "        > clf = LinearRegression()\n",
    "        > clf.fit(x_train, y_train)\n",
    "        > clf.predict(x_eval)\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.0001, max_iter=1000,\n",
    "                 theta_0=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            learning_rate: Learning rate for sgd\n",
    "            max_iter: Maximum number of iterations for the solver.\n",
    "            theta_0: Initial guess for theta. If None, use the zero vector.\n",
    "        \"\"\"\n",
    "        self.theta = theta_0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "     \n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        \"\"\"Run SGD to minimize the Squares Loss for Linear regression.\n",
    "\n",
    "        Args:\n",
    "            x: Training example inputs. Shape (n_examples, dim).\n",
    "            y: Training example labels. Shape (n_examples,).\n",
    "        \"\"\"\n",
    "        for _ in tqdm(range(self.max_iter)):\n",
    "            for obs, label in zip(x, y):\n",
    "                self.theta -= 2 * self.learning_rate * (self.predict(obs) - label) * obs\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Return predicted probabilities given new inputs x.\n",
    "\n",
    "        Args:\n",
    "            x: Inputs of shape (n_examples, dim).\n",
    "\n",
    "        Returns:\n",
    "            Outputs of shape (n_examples,).\n",
    "        \"\"\"\n",
    "        return np.dot(x, self.theta)\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        y_pred = np.sign(self.predict(x_test))\n",
    "        loss = np.sum((y_test - y_pred)**2)\n",
    "        return loss/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1159e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "resl = plot_loss([ 0.001,0.0001, 0.00001], [10, 100, 250, 500, 1000, 5000], x_train, y_train, x_test, y_test, classifier =\"LinearRegression\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab36bb2",
   "metadata": {},
   "source": [
    "=> The best learning rate for *Linear Regression* is then $\\eta = 0.0001$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e4375a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thetas = save_theta([10,100,1000,10000], x_train, y_train, classifier=\"LinearRegression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec5137a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_thetas(thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6655a92",
   "metadata": {},
   "source": [
    "### Pereptron algorihtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfc0731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d82acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\" Perceptron Algorithm\n",
    "\n",
    "    Example usage:\n",
    "        > clf = Perceptron()\n",
    "        > clf.fit(x_train, y_train)\n",
    "        > clf.predict(x_eval)\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.001, max_iter=1000, theta_0=None ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            learning_rate: Learning rate for sgd\n",
    "            max_iter: Maximum number of iterations for the solver.\n",
    "            theta_0: Initial guess for theta. If None, use the zero vector.\n",
    "        \"\"\"\n",
    "        self.theta = theta_0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "     \n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Training example inputs. Shape (n_examples, dim).\n",
    "            y: Training example labels. Shape (n_examples,).\n",
    "        \"\"\"\n",
    "        for _ in tqdm(range(self.max_iter)):\n",
    "            for obs, label in zip(x, y):\n",
    "                self.theta -= self.learning_rate * (self.predict(obs) - label) * obs\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Return predicted probabilities given new inputs x.\n",
    "\n",
    "        Args:\n",
    "            x: Inputs of shape (n_examples, dim).\n",
    "\n",
    "        Returns:\n",
    "            Outputs of shape (n_examples,).\n",
    "        \"\"\"\n",
    "        if np.dot(x, self.theta)>= 0:\n",
    "            prediction = 1\n",
    "        else:\n",
    "            prediction = -1\n",
    "            \n",
    "        return prediction\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        y_pred = np.fromiter(map(self.predict, x_test), dtype = float)\n",
    "        loss = np.sum(abs(y_test - y_pred))\n",
    "        return loss/(2*len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9f34b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resp = plot_loss([0.1, 0.01, 0.001,0.0001], [10, 100, 250, 500, 1000], x_train, y_train, x_test, y_test, classifier =\"Perceptron\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c509af54",
   "metadata": {},
   "source": [
    "=> We see that the loss of the *Perceptron* does **not depend** on the learning rate, given that all the curves are *overlapped*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec2b0a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thetas = save_theta([10,100,1000,10000], x_train, y_train, classifier=\"Perceptron\")\n",
    "show_thetas(thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45038452",
   "metadata": {},
   "source": [
    "### (6) - K Nearest Neighbors (KNN)\n",
    "\n",
    "**(b)** -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea59883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data without adding the intercept \n",
    "train_data = load_train()\n",
    "test_data = load_test()\n",
    "\n",
    "# Splitting into train and test data\n",
    "x_train = train_data[:, :-1]\n",
    "y_train = train_data[:, -1]\n",
    "x_test = test_data[:, :-1]\n",
    "y_test = test_data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406088f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc2e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    \"\"\"KNN algorithm\n",
    "\n",
    "    Example usage:\n",
    "        > clf = KNN(5, x_train, y_train)\n",
    "        > clf.predict(x_eval)\n",
    "    \"\"\"\n",
    "    def __init__(self, K, x_train, y_train):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            theta_0: Initial guess for theta. If None, use the zero vector.\n",
    "        \"\"\"\n",
    "        self.K = K\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "     \n",
    "    def distance(self, a, b):\n",
    "        return np.linalg.norm(a-b)\n",
    "    \n",
    "    def fit(self, x_train, y_train):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Return predicted probabilities given new inputs x.\n",
    "\n",
    "        Args:\n",
    "            x: Inputs of shape (n_examples, dim).\n",
    "\n",
    "        Returns:\n",
    "            Outputs of shape (n_examples,).\n",
    "        \"\"\"\n",
    "        distances = np.fromiter(map(lambda vec:self.distance(vec, x), self.x_train), dtype = float) \n",
    "        inds = np.argsort(distances)[:self.K]\n",
    "        prop_ones = sum(self.y_train[inds])/len(inds)\n",
    "        if prop_ones >= 0:\n",
    "            return 1\n",
    "        return -1\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        y_pred = []\n",
    "        for test_sample in x_test:\n",
    "            y_pred.append(self.predict(test_sample))\n",
    "            \n",
    "        y_pred = np.array(y_pred)\n",
    "        loss = np.sum(abs((y_test - y_pred)))/(2*len(y_test))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e0cbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_KNN( x_train, y_train, x_test, y_test):\n",
    "    loss = []\n",
    "    for K in tqdm(range(1,50,1)):\n",
    "        clf = KNN(K, x_train[1000:3000], y_train[1000:3000])\n",
    "        loss.append(clf.evaluate(x_test, y_test))\n",
    "    plt.plot(range(1,50,1), loss)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f29168c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title(\"Testing Loss\")\n",
    "plot_loss_KNN( x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad8fcdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title(\"Training Loss\")\n",
    "plot_loss_KNN( x_train, y_train, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9986de3c",
   "metadata": {},
   "source": [
    "###  (6-c)  K-fold cross-validation to calibrate k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb483f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def split_train_test():\n",
    "  \n",
    "    test = [[list(range(450 * i, 450 *(i+1))) + list(range(2250 + 450*i, 2250 + 450 * (i+1) )) + list(range(4500 + 450*i, 4500+ 450*(i+1)))]  for i in range(5)]\n",
    "    train = []\n",
    "    for subtest in test:\n",
    "        total = list(range(6750))\n",
    "        for idx in subtest:\n",
    "            total.remove(idx)\n",
    "            train.append(total)\n",
    "        \n",
    "    return zip(np.array(train), np.array(test))\n",
    "'''\n",
    "\n",
    "def split_train_test(data):\n",
    "     indices = list(range(data.shape[0]))\n",
    "     random.shuffle(indices)\n",
    "     step = data.shape[0]//5\n",
    "\n",
    "     for i in range(0, data.shape[0], step):\n",
    "         test_indices = [j for j in indices[i: min(i+step, data.shape[0])]]\n",
    "         train_indices = [j for j in indices if j not in test_indices]\n",
    "         yield test_indices, train_indices\n",
    "\n",
    "def cross_validation(dataset):\n",
    "    loss = []\n",
    "    for K in tqdm(range(1,50)):\n",
    "        loss_K = []\n",
    "        for train_idx, test_idx in split_train_test(dataset):\n",
    "            \n",
    "            train_data = dataset[train_idx]\n",
    "            test_data = dataset[test_idx]\n",
    "            \n",
    "            x_train = train_data[:, :-1]\n",
    "            y_train = train_data[:, -1]\n",
    "            x_test = test_data[:, :-1]\n",
    "            y_test = test_data[:, -1]\n",
    "            \n",
    "            clf = KNN(K, x_train[:100], y_train[:100])\n",
    "            #import ipdb; ipdb.set_trace()\n",
    "\n",
    "            loss_K.append(clf.evaluate(x_test, y_test))\n",
    "            \n",
    "        loss.append((loss_K))\n",
    "    loss = np.array(loss)\n",
    "    \n",
    "    best_K = loss.argmin()+1\n",
    "    \n",
    "    return best_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c3679f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "data = np.concatenate((train_data, test_data))\n",
    "random.shuffle(data)\n",
    "print(f\" The optimal K is :{cross_validation(data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cdf045",
   "metadata": {},
   "source": [
    " <u><h4>Result:</h4></u> \n",
    "=>  According to <em>cross validation</em> and the plot of the <em>loss curve</em>, we conclude that the optimal parameter $K$ for our KNN algorithm  is <b>12</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b29680",
   "metadata": {},
   "source": [
    "\n",
    "### (7)- MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e97399",
   "metadata": {},
   "source": [
    "The number of paramters needed to train the *MLP* is **25219** parameters where **24400** are weights and **819** biases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a402b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(28*28,), activation=\"relu\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470348e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Adapting the data\n",
    "y_train[4000:6000] = 2*np.ones(2000)\n",
    "y_test[500:750] = 2*np.ones(250)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "print(\"Training started..., this can take a while:\")\n",
    "\n",
    "#Fitting the model\n",
    "model.fit(x_train, y_train, epochs = 50)\n",
    "\n",
    "# Evaluation the model\n",
    "print(\"Final loss value:\", model.evaluate(x_test, y_test ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfbaa51",
   "metadata": {},
   "source": [
    "<u><h3>(8)- Results of the models:</h3></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e1fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "train_data, test_data = load_train(), load_test()\n",
    "\n",
    "# Importing train and test data\n",
    "x_train, y_train = train_data[:, :-1], train_data[:, -1]\n",
    "x_test, y_test = test_data[:, :-1], test_data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_one_loss(x_train, y_train, x_test, y_test, classifier):\n",
    "    if classifier == \"Linear Regression\":\n",
    "        clf = LinearRegression(theta_0 = np.zeros(785), max_iter = 5000, learning_rate= 0.0001)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        \n",
    "    elif classifier == \"Logistic Regression\":\n",
    "        clf = LogisticRegression(theta_0 = np.zeros(785), max_iter = 500, learning_rate= 0.01)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "    \n",
    "    elif classifier == \"Perceptron\":\n",
    "        clf = Perceptron(theta_0 = np.zeros(785), max_iter = 10000, learning_rate= 0.001)\n",
    "        clf.fit(x_train, y_train)\n",
    "    \n",
    "    elif classifier == \"KNN\":\n",
    "        clf = KNN(12, x_train, y_train)\n",
    "        return clf.evaluate(x_test, y_test)\n",
    "    \n",
    "    elif classifier == \"MLP\":\n",
    "        y_pred = np.array(list(map(np.argmax, model.predict(x_test))))\n",
    "        y_pred = y_pred % 2 \n",
    "        y_pred= np.array(list(map(lambda x: x-1 if (x == 0) else x, y_pred)))\n",
    "\n",
    "    return sum(abs(y_pred - y_test)/(2*len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e49ef2a",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e5d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Testing Loss: {zero_one_loss(x_train, y_train, x_test, y_test, 'Linear Regression')}\")\n",
    "print(f\"Training Loss: {zero_one_loss(x_train, y_train, x_train, y_train, 'Linear Regression')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120711be",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f4e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Testing Loss: {zero_one_loss(x_train, y_train, x_test, y_test, 'Logistic Regression')}\")\n",
    "print(f\"Training Loss: {zero_one_loss(x_train, y_train, x_train, y_train, 'Logistic Regression')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ce5af9",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6967d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Testing Loss: {zero_one_loss(x_train, y_train, x_test, y_test, 'Perceptron')}\")\n",
    "print(f\"Training Loss: {zero_one_loss(x_train, y_train, x_train, y_train, 'Perceptron')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d7c99",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8b59b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Testing Loss: {zero_one_loss(x_train, y_train, x_test, y_test, 'KNN')}\")\n",
    "print(f\"Training Loss: {zero_one_loss(x_train, y_train, x_train, y_train, 'KNN')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e237a3de",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4dc654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If ValueError appears, please reload the data, it is because y_test is still one hot encoded from the training of the neural network \n",
    "print(f\"Testing Loss: {zero_one_loss(x_train, y_train, x_test, y_test, 'MLP')}\")\n",
    "print(f\"Training Loss: {zero_one_loss(x_train, y_train, x_train, y_train, 'MLP')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1d6018",
   "metadata": {},
   "source": [
    "|  | Logistic Regression | OLS | Perceptron | KNN | MLP |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| Empirical Error (0-1 loss) | 0.044 | 0.121|  0.0518 |0.0385   | 0.0039|\n",
    "| Test Error (0-1 loss) |  0.0624|  0.132| 0.0986 | 0.0506 |0.038 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2fc6cf",
   "metadata": {},
   "source": [
    "<u><h3> Comment on the results :</h3></u> \n",
    "\n",
    "* The best algorithm for our classification problem is **MLP** with a test error of **0.038**, which is expected thanks to its ability to model non-linear relationships, which is not the case for **GLMs** (General Linear Models) like <b>Logistic Regression, Linear Regression</b> and the **Perceptron Algorithm**.\n",
    "\n",
    "\n",
    "* The least performing algorithm is **KNN**, with a test error of **0.424** (slightly better than the random algorihtm), which was expected since KNN only compares two images by taking into account the differences in the pixels' intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d58115",
   "metadata": {},
   "source": [
    "### (10-) Regularization of logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7307dacb",
   "metadata": {},
   "source": [
    "#### (a) -Test and training classification errors with the 0-1 loss associated with $\\hat{\\theta}_\\lambda$ as a function of $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe6138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_lambda(lambdas, norm='l2'):\n",
    "    clf = LogisticRegression(theta_0 = np.zeros(785), max_iter = 10, learning_rate= 0.01)\n",
    "    train_loss , test_loss = [], []\n",
    "    \n",
    "    if norm == \"l1\":\n",
    "        for lam in lambdas:\n",
    "            clf.fit_reg_l1(x_train, y_train, lam)\n",
    "            train_loss.append(clf.zero_one_loss(x_train, y_train))\n",
    "            test_loss.append(clf.zero_one_loss(x_test, y_test))\n",
    "    elif norm == \"l2\":\n",
    "        for lam in lambdas:\n",
    "            clf.fit_reg_l1(x_train, y_train, lam)\n",
    "            train_loss.append(clf.zero_one_loss(x_train, y_train))\n",
    "            test_loss.append(clf.zero_one_loss(x_test, y_test))\n",
    "            \n",
    "    # plotting the curves\n",
    "    plt.plot(lambdas, train_loss, label=\"Training Loss\")\n",
    "    plt.plot(lambdas, test_loss, label=\"Test Loss\")\n",
    "    \n",
    "    plt.title(f\"Training and Test Loss as functions of lambda for {norm} regularization\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaf2c01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0,0.0005,10)\n",
    "losses_l2  = test_train_lambda(lambdas, \"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab5c83e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0,0.0005,10)\n",
    "losses_l1  = test_train_lambda(lambdas, \"l1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809615e2",
   "metadata": {},
   "source": [
    "#### (b)- Best value for $\\lambda$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas[np.argmin(losses_l2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fea4dea",
   "metadata": {},
   "source": [
    "=> From the plots above, we deduce that the best value for lambda for the $l2$ norm, is arround $0.00013$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5eb2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas[np.argmin(losses_l1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a00c9a",
   "metadata": {},
   "source": [
    "=> From the plots above, we deduce that the best value for lambda for the $l1$ norm, is arround $0.00013$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98993b53",
   "metadata": {},
   "source": [
    "#### (b)- Images of the estimator $\\hat{\\theta}_\\lambda$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_lambdas = [ 0.0001, 0.00015, 0.0002, 0.00025]\n",
    "\n",
    "thetas_1 = []\n",
    "thetas_2 = []\n",
    "\n",
    "for lam in four_lambdas:\n",
    "    clf_l1 = LogisticRegression(theta_0 = np.zeros(785), max_iter = 5000, learning_rate= 0.001)\n",
    "    clf_l1.fit_reg_l1(x_train, y_train, lam)\n",
    "    thetas_1.append(clf_l1.theta)\n",
    "    \n",
    "    clf_l2 = LogisticRegression(theta_0 = np.zeros(785), max_iter = 5000, learning_rate= 0.001)\n",
    "    clf_l2.fit_reg_l2(x_train, y_train, lam)\n",
    "    thetas_2.append(clf_l2.theta)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14018abe",
   "metadata": {},
   "source": [
    "#### Images of the estimator $\\hat{\\theta}_\\lambda$ for the $l2$ norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d475f56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_thetas(thetas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24083efb",
   "metadata": {},
   "source": [
    "#### Images of the estimator $\\hat{\\theta}_\\lambda$ for the $l1$ norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdd2f42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_thetas(thetas_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
